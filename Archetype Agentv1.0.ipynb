{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMQajkbwTctO4dPsUKZI03L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iMac69/chat_agent/blob/master/Archetype%20Agentv1.0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Install and Import Dependencies\n",
        "## Ensure all necessary libraries are installed and imported for the project.\n",
        "### This code installs all the required Python packages using pip and imports them into the script. Libraries like streamlit are used for the UI, sentence-transformers for embeddings, and pinecone-client for vector database operations."
      ],
      "metadata": {
        "id": "JO5UMu1DYkj6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "M_-fjE0zZs0l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "-OfUE_IIbbpl",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7b2389d-3176-4c44-e1b2-ed4c044533b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.10/dist-packages (1.38.0)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n",
            "Requirement already satisfied: numpy<3,>=1.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.26.4)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (24.1)\n",
            "Requirement already satisfied: pandas<3,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.1.4)\n",
            "Requirement already satisfied: pillow<11,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (10.4.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (14.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.8.1)\n",
            "Requirement already satisfied: tenacity<9,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.12.2)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.1.43)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.3)\n",
            "Requirement already satisfied: watchdog<5,>=2.1.5 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.0.2)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.12.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.11)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2024.8.30)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.18.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.5)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.20.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.3.0->streamlit) (1.16.0)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.1.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.44.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.5)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.4.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.24.7)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (10.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (0.19.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: pinecone-client in /usr/local/lib/python3.10/dist-packages (5.0.1)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2024.8.30)\n",
            "Requirement already satisfied: pinecone-plugin-inference<2.0.0,>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (1.1.0)\n",
            "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (0.0.7)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (4.12.2)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2.2.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n",
            "Requirement already satisfied: gspread in /usr/local/lib/python3.10/dist-packages (6.0.2)\n",
            "Requirement already satisfied: google-auth>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from gspread) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from gspread) (1.2.1)\n",
            "Requirement already satisfied: StrEnum==0.4.15 in /usr/local/lib/python3.10/dist-packages (from gspread) (0.4.15)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.12.0->gspread) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.12.0->gspread) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.12.0->gspread) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib>=0.4.1->gspread) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.12.0->gspread) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (3.2.2)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (2024.8.30)\n",
            "Requirement already satisfied: google-auth in /usr/local/lib/python3.10/dist-packages (2.27.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth) (4.9)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth) (0.6.1)\n",
            "Requirement already satisfied: uuid in /usr/local/lib/python3.10/dist-packages (1.30)\n"
          ]
        }
      ],
      "source": [
        "# Install required libraries (if not already installed)\n",
        "!pip install streamlit\n",
        "!pip install sentence-transformers\n",
        "!pip install pinecone-client\n",
        "!pip install nltk\n",
        "!pip install gspread\n",
        "!pip install google-auth\n",
        "!pip install uuid\n",
        "\n",
        "# Import libraries\n",
        "import streamlit as st\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import pinecone\n",
        "import nltk\n",
        "import uuid\n",
        "import json\n",
        "import datetime\n",
        "import gspread\n",
        "from google.oauth2.service_account import Credentials\n",
        "from scipy.spatial.distance import cosine"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Prepare the Knowledge Base"
      ],
      "metadata": {
        "id": "wC1rf1-Uhfxv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Load and Chunk the Documents\n",
        "### Summary: Load the custom prompt and archetype playbooks, then chunk them into smaller pieces for efficient embedding and retrieval.\n",
        "### Explanation: This code processes each document, chunks it, and prepares a list of chunks with unique IDs and metadata, including the archetype name and chunk identifiers.\n"
      ],
      "metadata": {
        "id": "-ow_l5vahh9U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "import nltk # Make sure to import the nltk module\n",
        "\n",
        "nltk.download('punkt')  # Download NLTK data files if not already present\n",
        "\n",
        "def load_documents(folder_path):\n",
        "    documents = {}\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith('.txt'):\n",
        "            archetype_name = filename.replace('.txt', '')\n",
        "            with open(os.path.join(folder_path, filename), 'r') as file:\n",
        "                documents[archetype_name] = file.read()\n",
        "    return documents\n",
        "\n",
        "def chunk_document(text, max_words=300):\n",
        "    sentences = sent_tokenize(text)\n",
        "    chunks = []\n",
        "    current_chunk = ''\n",
        "    word_count = 0\n",
        "\n",
        "    for sentence in sentences:\n",
        "        words_in_sentence = len(sentence.split())\n",
        "        if word_count + words_in_sentence <= max_words:\n",
        "            current_chunk += ' ' + sentence\n",
        "            word_count += words_in_sentence\n",
        "        else:\n",
        "            chunks.append(current_chunk.strip())\n",
        "            current_chunk = sentence\n",
        "            word_count = words_in_sentence\n",
        "    if current_chunk:\n",
        "        chunks.append(current_chunk.strip())\n",
        "    return chunks"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbqBSS6Yhm5N",
        "outputId": "a5716c37-c390-4abf-eeb5-b817a4ee5d28"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.2 Prepare Chunks with Metadata\n",
        "Summary: Associate each chunk with relevant metadata for efficient retrieval."
      ],
      "metadata": {
        "id": "MxCkj00-kZw5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_chunks(documents):\n",
        "    all_chunks = []\n",
        "    for archetype_name, text in documents.items():\n",
        "        chunks = chunk_document(text)\n",
        "        for idx, chunk in enumerate(chunks):\n",
        "            chunk_data = {\n",
        "                'id': f\"{archetype_name}_{idx}\",\n",
        "                'text': chunk,\n",
        "                'metadata': {\n",
        "                    'archetype': archetype_name,\n",
        "                    'chunk_id': idx,\n",
        "                    'total_chunks': len(chunks)\n",
        "                }\n",
        "            }\n",
        "            all_chunks.append(chunk_data)\n",
        "    return all_chunks\n",
        "\n",
        "# Example usage\n",
        "folder_path = '/content/knowledge_base'\n",
        "documents = load_documents(folder_path)\n",
        "all_chunks = prepare_chunks(documents)\n"
      ],
      "metadata": {
        "id": "39dgZSS9kbQX"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Set Up Pinecone Indexing\n",
        "### Summary: Initialize Pinecone and create an index to store embeddings.\n",
        "### Explanation: This code initializes Pinecone with your API key, checks if the specified index exists, and creates it if necessary. The dimension parameter should match the embedding size of the model used."
      ],
      "metadata": {
        "id": "6ppIYLvKaX8t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.1 Access Environment Variables\n",
        "\n",
        "### Summary: Retrieve API keys and other sensitive information from environment variables.\n",
        "\n",
        "### Explanation: This code accesses the Pinecone API key and environment variables securely stored in Colab’s environment. Avoid hardcoding sensitive information directly in the script to maintain security."
      ],
      "metadata": {
        "id": "EuFuab3QcZni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Initialize and Configure Pinecone (Corrected)\n",
        "\n",
        "# Install the latest Pinecone client\n",
        "!pip install --upgrade pinecone-client\n",
        "\n",
        "import os\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from google.colab import userdata\n",
        "\n",
        "# Access secrets set via Colab's UI\n",
        "openai_api_key = userdata.get('OPENAI_API_KEY')      # Use userdata.get() to access secrets\n",
        "pinecone_api_key = userdata.get('PINECONE_API_KEY')  # Use userdata.get() to access secrets\n",
        "pinecone_env = userdata.get('PINECONE_ENVIRONMENT')  # Use userdata.get() to access secrets\n",
        "\n",
        "# Verify that the API keys are set\n",
        "if all([openai_api_key, pinecone_api_key, pinecone_env]):\n",
        "    print(\"All API keys are set successfully!\")\n",
        "else:\n",
        "    print(\"Error: One or more API keys are missing.\")\n",
        "\n",
        "# Import necessary classes from pinecone\n",
        "from pinecone import Pinecone, ServerlessSpec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "G-UxQJlScd5B",
        "outputId": "28c3e35d-c54b-4d40-ef2f-5969189dc02e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pinecone-client in /usr/local/lib/python3.10/dist-packages (5.0.1)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2024.8.30)\n",
            "Requirement already satisfied: pinecone-plugin-inference<2.0.0,>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (1.1.0)\n",
            "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (0.0.7)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (4.12.2)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2.2.3)\n",
            "All API keys are set successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.2 Initialize Pinecone Client and Create Index\n",
        "\n",
        "### Summary: Initialize the Pinecone client with the latest API method and create an index if it doesn’t already exist.\n",
        "### Explanation:\n",
        "\n",
        "\t•\tInitialization: Uses the updated pinecone.Client method to initialize the Pinecone client with your API key and environment.\n",
        "\t•\tIndex Creation: Checks if the specified index (archetype-index) exists. If not, it creates the index with a dimension of 384, which matches the embedding size of the all-MiniLM-L6-v2 model.\n",
        "\t•\tConnection: Establishes a connection to the Pinecone index for subsequent operations like upserting and querying vectors."
      ],
      "metadata": {
        "id": "9PulWL6qcp-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***# Add this updated code cell back to gpt to update coding inforamtion available to the model. ***"
      ],
      "metadata": {
        "id": "hT52_yoAcxkM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Pinecone with your API key and environment\n",
        "pc = Pinecone(\n",
        "    api_key=pinecone_api_key,    # Your Pinecone API key\n",
        "    environment=pinecone_env      # Your Pinecone environment (e.g., 'us-east1-gcp')\n",
        ")\n",
        "\n",
        "# Check existing indexes\n",
        "existing_indexes = pc.list_indexes().names()  # Correctly call the 'names' method\n",
        "print(f\"Existing Pinecone indexes: {existing_indexes}\")\n",
        "\n",
        "# Define your index name\n",
        "index_name = 'knowledge-base'\n",
        "\n",
        "# Create a new index if it doesn't exist\n",
        "if index_name not in existing_indexes:\n",
        "    pc.create_index(\n",
        "        name=index_name,\n",
        "        dimension=384,  # embedding size of the all-MiniLM-L6-v2 model\n",
        "        metric='cosine',\n",
        "        spec=ServerlessSpec(\n",
        "            cloud='aws',        # Choose your cloud provider ('aws', 'gcp', etc.)\n",
        "            region='us-east-1'   # Choose the appropriate region\n",
        "        )\n",
        "    )\n",
        "    print(f\"Created Pinecone index: {index_name}\")\n",
        "else:\n",
        "    print(f\"Pinecone index '{index_name}' already exists.\")\n",
        "\n",
        "# Connect to the index\n",
        "index = pc.Index(index_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "YQ5yqf5DakJo",
        "outputId": "0a0b4cd0-e3c8-429d-ba3f-c628cd0e8b75"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Existing Pinecone indexes: ['knowledge-base', 'archetype-playbook-index', 'archetype-playbooks', 'archetype-index', 'archetype-playbooks-index']\n",
            "Pinecone index 'knowledge-base' already exists.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Develop the Embedding and Retrieval Module"
      ],
      "metadata": {
        "id": "ixd8lYVogfIH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.1 Initialize the Embedding Model\n",
        "\n",
        "### Summary: Load the all-MiniLM-L6-v2 model for generating text embeddings."
      ],
      "metadata": {
        "id": "6x8G5ZnYgjgI"
      }
    },
    {
      "source": [
        "!pip install sentence-transformers\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Load the embedding model\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')"
      ],
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VldWvWmugxem",
        "outputId": "9be4585b-c197-477e-9785-805d5ece57cf"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.1.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.44.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.5)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.4.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.24.7)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (10.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (0.19.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explanation:\n",
        "This line loads the pre-trained Sentence Transformer model into memory for generating embeddings. The all-MiniLM-L6-v2 model provides a good balance between performance and efficiency, making it suitable for real-time applications."
      ],
      "metadata": {
        "id": "xSY5wbM2g77_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "dmOKErkthXQ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.2 Embed and Index the Chunks\n",
        "\n",
        "Summary: Generate embeddings for each chunk and upsert them into the Pinecone index."
      ],
      "metadata": {
        "id": "6X1qo8tyhgEO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the embedding model\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "def index_chunks(chunks):\n",
        "    \"\"\"\n",
        "    Generate embeddings for each chunk and upsert them into Pinecone.\n",
        "\n",
        "    Args:\n",
        "        chunks (list): List of chunk dictionaries with 'id', 'text', and 'metadata'.\n",
        "    \"\"\"\n",
        "    # Prepare a list of tuples for upsert\n",
        "    upsert_data = []\n",
        "    for chunk in chunks:\n",
        "        embedding = model.encode(chunk['text']).tolist()\n",
        "        upsert_data.append((chunk['id'], embedding, chunk['metadata']))\n",
        "\n",
        "    # Upsert all chunks in bulk for efficiency\n",
        "    # Check if upsert_data is not empty before attempting to upsert\n",
        "    if upsert_data:\n",
        "        index.upsert(vectors=upsert_data)\n",
        "\n",
        "# Index the chunks\n",
        "index_chunks(all_chunks)"
      ],
      "metadata": {
        "id": "6R7lKB8qlaKu"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# Explanation ADD\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VgY1V86Wm8c4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Implement the Conversation Manager\n",
        "###Summary: Create unique session tokens to manage individual client sessions."
      ],
      "metadata": {
        "id": "x5nW5JTsnGAb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5.1 Generate Session Tokens\n",
        "### Summary: Create unique session tokens to manage individual client sessions."
      ],
      "metadata": {
        "id": "2aHyWlRVnYA3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_session_token():\n",
        "    \"\"\"\n",
        "    Generate a unique session token using UUID4.\n",
        "\n",
        "    Returns:\n",
        "        str: A unique session token.\n",
        "    \"\"\"\n",
        "    return str(uuid.uuid4())"
      ],
      "metadata": {
        "id": "hz8S5xD9nLB8"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explanation:\n",
        "This function generates a unique identifier for each client session using UUID4, ensuring that each interview is uniquely tracked and attributed.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "8bgBMXOMnQ2-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5.2 Define the Conversation Flow\n",
        "\n",
        "### Summary: Manage the flow of the interview, including questions and response handling."
      ],
      "metadata": {
        "id": "Kvn8_Xl3nf0P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the list of questions and follow-ups\n",
        "questions = [\n",
        "    {\n",
        "        'question': \"What’s your primary goal in interacting with customers?\",\n",
        "        'follow_up': \"Can you give a specific example?\"\n",
        "    },\n",
        "    {\n",
        "        'question': \"How would you describe your ideal brand voice?\",\n",
        "        'follow_up': \"Does it vary by platform or audience?\"\n",
        "    },\n",
        "    # Add other questions as needed\n",
        "]\n",
        "\n",
        "def interview_flow():\n",
        "    \"\"\"\n",
        "    Manage the interview flow by presenting questions and capturing responses.\n",
        "    \"\"\"\n",
        "    if 'session_token' not in st.session_state:\n",
        "        st.session_state['session_token'] = generate_session_token()\n",
        "    if 'responses' not in st.session_state:\n",
        "        st.session_state['responses'] = []\n",
        "\n",
        "    st.write(\"Hi there! I'm excited to learn more about your brand. 😊\")\n",
        "\n",
        "    for idx, q in enumerate(questions):\n",
        "        with st.expander(f\"Question {idx + 1}\"):\n",
        "            response = st.text_input(q['question'], key=f\"q_{idx}\")\n",
        "            follow_up = st.text_input(q['follow_up'], key=f\"q_{idx}_follow\")\n",
        "            if response and follow_up:\n",
        "                st.session_state['responses'].append({\n",
        "                    'question': q['question'],\n",
        "                    'follow_up': q['follow_up'],\n",
        "                    'response': {\n",
        "                        'answer': response,\n",
        "                        'example': follow_up\n",
        "                    }\n",
        "                })"
      ],
      "metadata": {
        "id": "zWez2TOInlzN"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explanation:\n",
        "\n",
        "\t•\tQuestions Definition: A list of dictionaries containing each question and its corresponding follow-up question.\n",
        "\t•\tinterview_flow Function: Initializes session variables if they don’t exist and iterates through each question, capturing user responses. Uses Streamlit’s expander for better UI organization, allowing users to focus on one question at a time.**bold text**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "m1nqA5Rznp6I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6: Build the Classification Engine\n",
        "\n",
        "Summary: Calculate similarity scores and determine primary and secondary archetypes based on client responses."
      ],
      "metadata": {
        "id": "n99BXixEnxnw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_archetype_embedding(archetype_name):\n",
        "    \"\"\"\n",
        "    Retrieve and average embeddings for a given archetype.\n",
        "\n",
        "    Args:\n",
        "        archetype_name (str): The name of the archetype.\n",
        "\n",
        "    Returns:\n",
        "        list: Averaged embedding vector for the archetype.\n",
        "    \"\"\"\n",
        "    # Query Pinecone for all chunks related to the archetype\n",
        "    query_result = index.query(filter={'archetype': archetype_name}, top=100, include_values=True)\n",
        "\n",
        "    embeddings = [match['values'] for match in query_result['matches']]\n",
        "    if not embeddings:\n",
        "        return [0.0] * 384  # Return a zero vector if no embeddings found\n",
        "\n",
        "    # Calculate the average embedding\n",
        "    archetype_embedding = [sum(col) / len(col) for col in zip(*embeddings)]\n",
        "    return archetype_embedding\n",
        "\n",
        "def classify_archetypes(responses):\n",
        "    \"\"\"\n",
        "    Classify the client into primary and secondary archetypes based on responses.\n",
        "\n",
        "    Args:\n",
        "        responses (list): List of response dictionaries.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Primary archetype and secondary archetype (if any).\n",
        "    \"\"\"\n",
        "    # Initialize a dictionary to hold cumulative similarity scores\n",
        "    archetype_scores = {archetype: 0 for archetype in documents.keys()}\n",
        "\n",
        "    for response in responses:\n",
        "        response_text = response['response']['answer'] + ' ' + response['response']['example']\n",
        "        response_embedding = model.encode(response_text)\n",
        "\n",
        "        for archetype in archetype_scores.keys():\n",
        "            archetype_embedding = get_archetype_embedding(archetype)\n",
        "            score = 1 - cosine(response_embedding, archetype_embedding)\n",
        "            archetype_scores[archetype] += score\n",
        "\n",
        "    # Sort archetypes based on cumulative scores\n",
        "    sorted_archetypes = sorted(archetype_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    primary_archetype = sorted_archetypes[0][0]\n",
        "    # Define threshold for secondary archetype (e.g., within 10% of primary score)\n",
        "    threshold = 0.9 * sorted_archetypes[0][1]\n",
        "\n",
        "    secondary_archetype = sorted_archetypes[1][0] if sorted_archetypes[1][1] > threshold else None\n",
        "\n",
        "    return primary_archetype, secondary_archetype"
      ],
      "metadata": {
        "id": "q0UIRFngn1Yb"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explanation:\n",
        "\n",
        "\t•\tget_archetype_embedding Function: Retrieves all embeddings associated with a specific archetype from Pinecone and calculates their average to form a representative embedding vector for the archetype. If no embeddings are found, it returns a zero vector to avoid errors in similarity calculations.\n",
        "\t•\tclassify_archetypes Function:\n",
        "\t•\tIterates through each client response, generates its embedding, and calculates the cosine similarity with each archetype’s average embedding.\n",
        "\t•\tAggregates the similarity scores for each archetype across all responses.\n",
        "\t•\tDetermines the primary archetype as the one with the highest cumulative score.\n",
        "\t•\tIdentifies a secondary archetype if its score is within 90% of the primary archetype’s score.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "_mtq6cHdn5dt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 7: Design the User Interface"
      ],
      "metadata": {
        "id": "7k04SxVtoB55"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7.1 Customize the Streamlit App\n",
        "\n",
        "Summary: Set up the Streamlit page configuration and apply custom theming."
      ],
      "metadata": {
        "id": "v-NWLrW5oFde"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply custom CSS for branding\n",
        "def local_css(file_name):\n",
        "    with open(file_name) as f:\n",
        "        st.markdown(f'<style>{f.read()}</style>', unsafe_allow_html=True)\n",
        "\n",
        "local_css(\"/content/UI/styles.css\")\n",
        "\n",
        "# Streamlit Components\n",
        "st.title(\"Tedesco AI Automation Solutions\")\n",
        "st.button(\"Learn More\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRzXVJALoI4T",
        "outputId": "aed793b3-7c15-4429-a549-b26640d3bc58"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-09-25 00:48:15.329 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-09-25 00:48:15.518 \n",
            "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
            "  command:\n",
            "\n",
            "    streamlit run /usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py [ARGUMENTS]\n",
            "2024-09-25 00:48:15.522 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-09-25 00:48:15.524 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-09-25 00:48:15.526 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-09-25 00:48:15.530 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-09-25 00:48:15.532 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-09-25 00:48:15.536 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-09-25 00:48:15.537 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explanation:\n",
        "\n",
        "\t•\tPage Configuration: Sets the title, icon, and layout of the Streamlit app.\n",
        "\t•\tCustom CSS: Applies custom styles from a styles.css file to match your website’s branding. Ensure that the styles.css file is present in your project directory and contains the necessary CSS rules for colors, fonts, logos, and layout.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "C5cTQbNCoMRB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7.2 Create the Chat Interface\n",
        "\n",
        "Summary: Build the chat-like interface for the interview."
      ],
      "metadata": {
        "id": "ygtWqWxtoS5C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chat_interface():\n",
        "    \"\"\"\n",
        "    Render the chat interface for the interview.\n",
        "    \"\"\"\n",
        "    st.title(\"Brand Archetype Interview\")\n",
        "    interview_flow()"
      ],
      "metadata": {
        "id": "BfeVZxuqoUxq"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explanation:\n",
        "This function sets the title of the Streamlit app and invokes the interview_flow function to manage the conversation with the client.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "IWDV-T02oXWO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7.3 Display Archetype Summaries\n",
        "\n",
        "Summary: Present the client’s primary and secondary archetypes with brief summaries."
      ],
      "metadata": {
        "id": "FuqgDNbVoarS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_archetype_summary(archetype_name):\n",
        "    \"\"\"\n",
        "    Retrieve a summary for a given archetype.\n",
        "\n",
        "    Args:\n",
        "        archetype_name (str): The name of the archetype.\n",
        "\n",
        "    Returns:\n",
        "        str: A brief summary of the archetype.\n",
        "    \"\"\"\n",
        "    # For simplicity, return the first 200 characters from the archetype document\n",
        "    return documents[archetype_name][:200] + \"...\"\n",
        "\n",
        "def display_archetype_summary(primary, secondary):\n",
        "    \"\"\"\n",
        "    Display the primary and secondary archetype summaries to the client.\n",
        "\n",
        "    Args:\n",
        "        primary (str): Primary archetype.\n",
        "        secondary (str or None): Secondary archetype, if any.\n",
        "    \"\"\"\n",
        "    st.header(\"Interview Results\")\n",
        "\n",
        "    st.subheader(f\"Primary Archetype: {primary}\")\n",
        "    st.write(get_archetype_summary(primary))\n",
        "\n",
        "    if secondary:\n",
        "        st.subheader(f\"Secondary Archetype: {secondary}\")\n",
        "        st.write(get_archetype_summary(secondary))"
      ],
      "metadata": {
        "id": "Nif-6Rk3odYI"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explanation:\n",
        "\n",
        "\t•\tget_archetype_summary Function: Retrieves a brief summary of the specified archetype. Here, it simply returns the first 200 characters of the archetype’s playbook text. You can enhance this by providing more structured summaries.\n",
        "\t•\tdisplay_archetype_summary Function: Displays the primary and, if applicable, secondary archetypes along with their summaries to the client.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "wCV_yQjtogwc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 8: Implement Data Storage and Export\n"
      ],
      "metadata": {
        "id": "ugf_g3btoldI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8.1 Create Transcript Structure\n",
        "\n",
        "Summary: Structure the conversation data into a JSON format."
      ],
      "metadata": {
        "id": "w518jzmFon9L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_transcript(responses, primary, secondary):\n",
        "    \"\"\"\n",
        "    Create a structured transcript of the interview.\n",
        "\n",
        "    Args:\n",
        "        responses (list): List of response dictionaries.\n",
        "        primary (str): Primary archetype.\n",
        "        secondary (str or None): Secondary archetype.\n",
        "\n",
        "    Returns:\n",
        "        dict: A structured transcript dictionary.\n",
        "    \"\"\"\n",
        "    transcript = {\n",
        "        \"session_token\": st.session_state['session_token'],\n",
        "        \"timestamp\": datetime.datetime.utcnow().isoformat(),\n",
        "        \"interview\": responses,\n",
        "        \"archetypes\": {\n",
        "            \"primary\": primary,\n",
        "            \"secondary\": secondary\n",
        "        }\n",
        "    }\n",
        "    return transcript"
      ],
      "metadata": {
        "id": "GZi6A953ophk"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explanation:\n",
        "This function compiles all the interview data into a structured JSON object, including the session token, timestamp, list of questions and responses, and the identified archetypes.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "ZSNmEHuLosrv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8.2 Export Transcript to Google Sheets\n",
        "\n",
        "Summary: Parse the JSON transcript and export it to Google Sheets in a consistent format."
      ],
      "metadata": {
        "id": "V6h5L8oKow4O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def export_to_sheets(transcript):\n",
        "    \"\"\"\n",
        "    Export the transcript to Google Sheets.\n",
        "\n",
        "    Args:\n",
        "        transcript (dict): The structured transcript dictionary.\n",
        "    \"\"\"\n",
        "    # Define the scope for Google Sheets API\n",
        "    scope = ['https://www.googleapis.com/auth/spreadsheets']\n",
        "\n",
        "    # Load Google Sheets credentials from environment variable\n",
        "    GOOGLE_SHEETS_CREDENTIALS = os.getenv('GOOGLE_SHEETS_CREDENTIALS')  # JSON credentials as a string\n",
        "\n",
        "    # Write the credentials to a temporary file\n",
        "    with open('credentials.json', 'w') as f:\n",
        "        f.write(GOOGLE_SHEETS_CREDENTIALS)\n",
        "\n",
        "    # Authenticate and create the client\n",
        "    creds = Credentials.from_service_account_file('credentials.json', scopes=scope)\n",
        "    client = gspread.authorize(creds)\n",
        "\n",
        "    # Open the Google Sheet (replace 'Transcripts' with your sheet name)\n",
        "    sheet = client.open('Transcripts').sheet1\n",
        "\n",
        "    # Prepare data for insertion\n",
        "    row = [\n",
        "        transcript['session_token'],\n",
        "        transcript['timestamp']\n",
        "    ]\n",
        "    for entry in transcript['interview']:\n",
        "        row.extend([\n",
        "            entry['question'],\n",
        "            entry['response']['answer'],\n",
        "            entry['follow_up'],\n",
        "            entry['response']['example']\n",
        "        ])\n",
        "    row.extend([\n",
        "        transcript['archetypes']['primary'],\n",
        "        transcript['archetypes']['secondary'] if transcript['archetypes']['secondary'] else ''\n",
        "    ])\n",
        "\n",
        "    # Insert the row into the sheet\n",
        "    sheet.append_row(row)"
      ],
      "metadata": {
        "id": "Gu-o6IG-oyTP"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explanation:\n",
        "\n",
        "\t•\tGoogle Sheets Authentication:\n",
        "\t•\tRetrieves Google Sheets credentials from an environment variable (GOOGLE_SHEETS_CREDENTIALS). Ensure that this variable contains the JSON credentials as a string.\n",
        "\t•\tWrites the credentials to a temporary credentials.json file to authenticate with the Google Sheets API using gspread.\n",
        "\t•\tExporting Data:\n",
        "\t•\tOpens the specified Google Sheet (Transcripts).\n",
        "\t•\tPrepares the data by flattening the JSON structure into a single row, including session token, timestamp, questions, responses, follow-ups, examples, and identified archetypes.\n",
        "\t•\tAppends the row to the Google Sheet.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "1q7lUfKGo2XH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 9: Integrate All Components\n",
        "Summary: Combine all modules into the main application script."
      ],
      "metadata": {
        "id": "SgCuoerTo6vo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    \"\"\"\n",
        "    Orchestrate the overall application flow.\n",
        "    \"\"\"\n",
        "    chat_interface()\n",
        "\n",
        "    if st.button(\"Submit\", key='submit'):\n",
        "        if 'responses' in st.session_state and st.session_state['responses']:\n",
        "            primary, secondary = classify_archetypes(st.session_state['responses'])\n",
        "            display_archetype_summary(primary, secondary)\n",
        "            transcript = create_transcript(st.session_state['responses'], primary, secondary)\n",
        "            export_to_sheets(transcript)\n",
        "            st.success(\"Your archetypes have been identified and the transcript has been saved!\")\n",
        "        else:\n",
        "            st.warning(\"Please answer all questions before submitting.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-g5hCvxpBCJ",
        "outputId": "9ce4f1d0-8d23-45fb-a052-6c4834eb41cc"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-09-25 00:48:51.064 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-09-25 00:48:51.067 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-09-25 00:48:51.069 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-09-25 00:48:51.072 Session state does not function when running a script without `streamlit run`\n",
            "2024-09-25 00:48:51.074 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-09-25 00:48:51.075 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-09-25 00:48:51.076 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-09-25 00:48:51.077 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-09-25 00:48:51.078 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-09-25 00:48:51.082 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-09-25 00:48:51.084 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-09-25 00:48:51.086 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-09-25 00:48:51.088 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-09-25 00:48:51.089 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-09-25 00:48:51.091 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-09-25 00:48:51.094 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-09-25 00:48:51.095 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-09-25 00:48:51.096 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-09-25 00:48:51.099 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-09-25 00:48:51.100 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-09-25 00:48:51.102 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-09-25 00:48:51.103 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-09-25 00:48:51.104 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-09-25 00:48:51.107 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-09-25 00:48:51.108 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-09-25 00:48:51.109 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-09-25 00:48:51.111 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-09-25 00:48:51.114 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-09-25 00:48:51.115 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-09-25 00:48:51.117 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-09-25 00:48:51.118 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-09-25 00:48:51.120 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-09-25 00:48:51.121 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-09-25 00:48:51.123 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-09-25 00:48:51.126 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-09-25 00:48:51.128 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-09-25 00:48:51.130 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-09-25 00:48:51.131 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-09-25 00:48:51.132 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explanation:\n",
        "\n",
        "\t•\tmain Function:\n",
        "\t•\tInitiates the chat interface where the client answers the interview questions.\n",
        "\t•\tUpon clicking the “Submit” button:\n",
        "\t•\tChecks if responses exist in the session state.\n",
        "\t•\tPerforms archetype classification based on the responses.\n",
        "\t•\tDisplays the identified archetypes with summaries.\n",
        "\t•\tCreates a structured transcript and exports it to Google Sheets.\n",
        "\t•\tProvides feedback to the user about the successful completion of the process.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "-dxqUmdhpD5i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.1 Testing the Application\n",
        "Summary: Run the application locally to test all functionalities."
      ],
      "metadata": {
        "id": "e9wCyydoqD7Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGO57GYKwO36",
        "outputId": "d4e3f6f7-88d4-4050-b83a-6dce6694be3f"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.2.0-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "public_url = ngrok.connect(8501)\n",
        "print(public_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "TJLMDm80wTTH",
        "outputId": "db0e995f-8040-4a13-9204-15e8a8adade6"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:pyngrok.process.ngrok:t=2024-09-25T01:17:56+0000 lvl=eror msg=\"failed to reconnect session\" obj=tunnels.session err=\"authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n\"\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "PyngrokNgrokError",
          "evalue": "The ngrok process errored on start: authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPyngrokNgrokError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-71-46c9e48e9bfa>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyngrok\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mngrok\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpublic_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mngrok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8501\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpublic_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyngrok/ngrok.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(addr, proto, name, pyngrok_config, **options)\u001b[0m\n\u001b[1;32m    314\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"auth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m     \u001b[0mapi_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_ngrok_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_url\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Creating tunnel with options: {options}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyngrok/ngrok.py\u001b[0m in \u001b[0;36mget_ngrok_process\u001b[0;34m(pyngrok_config)\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0minstall_ngrok\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyngrok/process.py\u001b[0m in \u001b[0;36mget_process\u001b[0;34m(pyngrok_config)\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_current_processes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mngrok_path\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_start_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyngrok/process.py\u001b[0m in \u001b[0;36m_start_process\u001b[0;34m(pyngrok_config)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mngrok_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartup_error\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m             raise PyngrokNgrokError(f\"The ngrok process errored on start: {ngrok_process.startup_error}.\",\n\u001b[0m\u001b[1;32m    399\u001b[0m                                     \u001b[0mngrok_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m                                     ngrok_process.startup_error)\n",
            "\u001b[0;31mPyngrokNgrokError\u001b[0m: The ngrok process errored on start: authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run /usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3FBdjxO3uB0O",
        "outputId": "fa70112f-8e22-4b06-fdac-b46cc17fb4ba"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://35.196.24.72:8501\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "\u001b[34m  Stopping...\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-s-2huY6wNqa"
      }
    }
  ]
}